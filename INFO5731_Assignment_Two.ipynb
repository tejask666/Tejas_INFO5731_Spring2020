{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of INFO5731_Assignment_Two.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejask666/Tejas_INFO5731_Spring2020/blob/master/INFO5731_Assignment_Two.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9",
        "colab_type": "text"
      },
      "source": [
        "# **INFO5731 Assignment Two**\n",
        "\n",
        "In this assignment, you will try to gather text data from open data source via web scraping or API. After that you need to clean the text data and syntactic analysis of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF",
        "colab_type": "text"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k",
        "colab_type": "text"
      },
      "source": [
        "(40 points). Write a python program to collect text data from **either of the following sources** and save the data into a **csv file**:\n",
        "\n",
        "(1) Collect all the customer reviews of the product [2019 Dell labtop](https://www.amazon.com/Dell-Inspiron-5000-5570-Laptop/dp/B07N49F51N/ref=sr_1_11?crid=1IJ7UWF2F4GHH&keywords=dell%2Bxps%2B15&qid=1580173569&sprefix=dell%2Caps%2C181&sr=8-11&th=1) on amazon.\n",
        "\n",
        "(2) Collect the top 100 User Reviews of the film [Joker](https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv) from IMDB.\n",
        "\n",
        "(3) Collect the abstracts of the top 100 research papers by using the query [natural language processing](https://citeseerx.ist.psu.edu/search?q=natural+language+processing&submit.x=0&submit.y=0&sort=rlv&t=doc) from CiteSeerX.\n",
        "\n",
        "(4) Collect the top 100 tweets by using hashtag [\"#wuhancoronovirus\"](https://twitter.com/hashtag/wuhancoronovirus) from Twitter. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z",
        "colab_type": "text"
      },
      "source": [
        "# **Question 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw",
        "colab_type": "text"
      },
      "source": [
        "(30 points). Write a python program to **clean the text data** you collected above and save the data in a new column in the csv file. The data cleaning steps include:\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming. \n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV",
        "colab_type": "text"
      },
      "source": [
        "# **Question 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX",
        "colab_type": "text"
      },
      "source": [
        "(30 points). Write a python program to conduct **syntax and structure analysis** of the clean text you just saved above. The syntax and structure analysis includes: \n",
        "\n",
        "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFX71HUGVl3v",
        "colab_type": "code",
        "outputId": "e561c76a-cde1-416e-d32d-6a49025bbbdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "    # set options to be headless, ..\n",
        "    !apt-get update\n",
        "    import os\n",
        "    !apt install chromium-chromedriver\n",
        "    !cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "    !pip install selenium\n",
        "    from selenium import webdriver\n",
        "    from selenium.webdriver import ActionChains\n",
        "    from selenium.webdriver.support.wait import WebDriverWait\n",
        "    from selenium.webdriver.support import expected_conditions as ec\n",
        "    import time\n",
        "    import spacy\n",
        "    from spacy import displacy\n",
        "    import benepar\n",
        "    from pandas import DataFrame\n",
        "    import nltk\n",
        "    from nltk.stem import WordNetLemmatizer\n",
        "    from nltk.stem import PorterStemmer\n",
        "    from nltk.tokenize import word_tokenize\n",
        "    from google.colab import files\n",
        "    \n",
        "    class Assignment1():\n",
        "        def get_data_in_csv(self):\n",
        "             nltk.download('punkt')\n",
        "             nltk.download('wordnet')\n",
        "             options = webdriver.ChromeOptions()\n",
        "             options.add_argument('--headless')\n",
        "             options.add_argument('--no-sandbox')\n",
        "             options.add_argument('--disable-dev-shm-usage')\n",
        "             driver = webdriver.Chrome('chromedriver',options=options)\n",
        "             driver.get(\"https://www.imdb.com/title/tt7286456/\")\n",
        "             driver.find_element_by_xpath(\"//a[text()='USER REVIEWS']\").click()\n",
        "             for i in range(0, 3):\n",
        "                time.sleep(15)\n",
        "                loadmorebutton = driver.find_element_by_xpath(\"//button[@id='load-more-trigger']\")\n",
        "                driver.execute_script(\"arguments[0].scrollIntoView();\", loadmorebutton)\n",
        "                oActions = ActionChains(driver).move_to_element(loadmorebutton)\n",
        "                oActions.perform()\n",
        "                loadmorebutton.click()\n",
        "                time.sleep(5)\n",
        "             list = driver.find_elements_by_xpath('//div[@class=\"review-container\"]')\n",
        "             for i in range(0,100):\n",
        "                driver.execute_script(\"arguments[0].scrollIntoView();\", list[i])\n",
        "                wait = WebDriverWait(driver, 10)\n",
        "                wait.until(ec.visibility_of(list[i]))\n",
        "                content = list[i].text\n",
        "                A = Assignment1()\n",
        "                A.writeintocsv(content, i)\n",
        "    \n",
        "        def writeintocsv(self, text, i):\n",
        "            A = Assignment1()\n",
        "            removedpun = A.removeallpunctuation(text)\n",
        "            cleaned_data = A.lemma_and_stem(removedpun)\n",
        "            dataset = {'data': text, 'cleaned_data': cleaned_data}\n",
        "            df = DataFrame(dataset, columns=['data', 'cleaned_data'], index=[i])\n",
        "            export_csv = df.loc[[i]].to_csv('testit.csv', index=None,header=0,mode='a')  \n",
        "    \n",
        "        def removeallpunctuation(self, textstring):\n",
        "            textstring = textstring.lower()\n",
        "            temp = ''\n",
        "            removetext = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~0123456789'''\n",
        "            for i in range(len(textstring)):\n",
        "                if textstring[i] in removetext:\n",
        "                    pass\n",
        "                else:\n",
        "                    temp = temp + textstring[i]\n",
        "            return temp\n",
        "    \n",
        "        def cleansentence(self, sentence):\n",
        "            temp = ''\n",
        "            textlist = word_tokenize(sentence)\n",
        "            stopwordlist = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\",\n",
        "                            \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\",\n",
        "                            \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\",\n",
        "                            \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\",\n",
        "                            \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\",\n",
        "                            \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\",\n",
        "                            \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\",\n",
        "                            \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\",\n",
        "                            \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\",\n",
        "                            \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\",\n",
        "                            \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
        "            for i in range(len(textlist)):\n",
        "                if textlist[i] in stopwordlist:\n",
        "                    pass\n",
        "                else:\n",
        "                    temp = temp + \" \" + textlist[i]\n",
        "            return temp\n",
        "    \n",
        "        def lemma_and_stem(self,sentence):\n",
        "            temp=''\n",
        "            A=Assignment1()\n",
        "            text=A.cleansentence(sentence)\n",
        "            nlp = spacy.load(\"en_core_web_sm\")\n",
        "            doc = nlp(text)\n",
        "            for token in doc:\n",
        "                ps = PorterStemmer()\n",
        "                word=ps.stem(token.lemma_)\n",
        "                temp = temp + \" \" + word\n",
        "            return temp\n",
        "    \n",
        "        def count_named_entity_recognition(self,text):\n",
        "            person,org,location,product,date=0,0,0,0,0\n",
        "            nlp = spacy.load('en_core_web_sm')\n",
        "            doc = nlp(text)\n",
        "            for ent in doc.ents:\n",
        "                if ent.label_=='PERSON':\n",
        "                    person=person+1\n",
        "                elif ent.label_=='ORG':\n",
        "                    org=org+1\n",
        "                elif ent.label_=='LOC' or ent.label_=='GPE':\n",
        "                    location=location+1\n",
        "                elif ent.label_=='PRODUCT':\n",
        "                    product=product+1\n",
        "                elif ent.label_=='DATE':\n",
        "                    date=date+1\n",
        "            print(\"Total number of persons: \"+str(person)+\",organization: \"+str(org)+\",location: \"+str(location)+\",product: \"+str(product)+\",date:\"+str(date))\n",
        "    \n",
        "        def tagpartofspeech(self,cleanedtext):\n",
        "                nlp = spacy.load(\"en_core_web_sm\")\n",
        "                doc = nlp(cleanedtext)\n",
        "                noun_count,verb_count,adverb_count,adj_count=0,0,0,0\n",
        "                for token in doc:\n",
        "                   if token.pos_ == 'NOUN':\n",
        "                    noun_count=noun_count+1\n",
        "                   elif token.pos_ == 'ADJ':\n",
        "                       adj_count=adj_count+1\n",
        "                   elif token.pos_ == 'VERB':\n",
        "                       verb_count=verb_count+1\n",
        "                   elif token.pos_ == 'ADV':\n",
        "                       adverb_count=adverb_count+1\n",
        "                print(\"Total number of nouns: \"+str(noun_count)+\",adjective: \"+str(adj_count)+\",adverb: \"+str(adverb_count)+\",verb: \"+str(verb_count))\n",
        "        def contituencyparse(self,sentence):\n",
        "                #nltk.download('punkt')\n",
        "                benepar.download('benepar_en2')\n",
        "                parser = benepar.Parser(\"benepar_en2\")\n",
        "                tree = parser.parse(sentence)\n",
        "                print(\"Contituency parsed representation of sentence:\")\n",
        "                print(tree)\n",
        "        def dependencyparse(self,sentence):\n",
        "                nlp = spacy.load('en')\n",
        "                doc = nlp(sentence)\n",
        "                print(\"Dependecy parsed representation of sentence:\")\n",
        "                for token in doc:\n",
        "                  print(\"{0}/{1} <--{2}-- {3}/{4}\".format(token.text, token.tag_, token.dep_, token.head.text, token.head.tag_))\n",
        "                displacy.render(doc, style='dep', jupyter=True, options={'distance': 90})\n",
        "    \n",
        "    A = Assignment1()\n",
        "    A.get_data_in_csv()\n",
        "    A.count_named_entity_recognition(\"one good comic book movie perform phoenix make speechless end\")\n",
        "    A.tagpartofspeech(\"one good comic book movie perform phoenix make speechless end\")\n",
        "    A.contituencyparse(\"one good comic book movie perform phoenix make speechless end\")\n",
        "    A.dependencyparse(\"one good comic book movie perform phoenix make speechless end\")\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (91.18\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com] [Connecting to\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com] [Connecting to\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com] [Connecting to\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.149)\r                                                                               \rHit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.149)\r0% [Connecting to archive.ubuntu.com (91.189.88.149)] [Waiting for headers] [Wa\r0% [Release.gpg gpgv 564 B] [Connecting to archive.ubuntu.com (91.189.88.149)] \r                                                                               \rHit:6 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:7 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:11 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Fetched 252 kB in 2s (150 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "chromium-chromedriver is already the newest version (80.0.3987.87-0ubuntu0.18.04.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 102 not upgraded.\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.6/dist-packages (3.141.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Total number of persons: 0,organization: 0,location: 0,product: 0,date:0\n",
            "Total number of nouns: 5,adjective: 3,adverb: 0,verb: 1\n",
            "[nltk_data] Downloading package benepar_en2 to /root/nltk_data...\n",
            "[nltk_data]   Package benepar_en2 is already up-to-date!\n",
            "Contituency parsed representation of sentence:\n",
            "(S\n",
            "  (NP\n",
            "    (NP (CD one) (JJ good) (JJ comic) (NN book) (NN movie))\n",
            "    (VB perform)\n",
            "    (NP (NNP phoenix)))\n",
            "  (VP (VB make) (S (NNP speechless) (VB end))))\n",
            "Dependecy parsed representation of sentence:\n",
            "one/CD <--nummod-- movie/NN\n",
            "good/JJ <--amod-- movie/NN\n",
            "comic/JJ <--amod-- book/NN\n",
            "book/NN <--compound-- movie/NN\n",
            "movie/NN <--nsubj-- perform/NN\n",
            "perform/NN <--compound-- phoenix/NN\n",
            "phoenix/NN <--nsubj-- make/VBP\n",
            "make/VBP <--ROOT-- make/VBP\n",
            "speechless/JJ <--compound-- end/NN\n",
            "end/NN <--dobj-- make/VBP\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"dfa5a3b951a3459fac44b950b178d635-0\" class=\"displacy\" width=\"950\" height=\"317.0\" direction=\"ltr\" style=\"max-width: none; height: 317.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">one</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NUM</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"140\">good</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"140\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"230\">comic</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"230\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"320\">book</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"320\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">movie</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">perform</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"590\">phoenix</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"590\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"680\">make</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"680\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">speechless</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"860\">end</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"860\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-dfa5a3b951a3459fac44b950b178d635-0-0\" stroke-width=\"2px\" d=\"M70,182.0 C70,2.0 410.0,2.0 410.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-dfa5a3b951a3459fac44b950b178d635-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,184.0 L62,172.0 78,172.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-dfa5a3b951a3459fac44b950b178d635-0-1\" stroke-width=\"2px\" d=\"M160,182.0 C160,47.0 405.0,47.0 405.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-dfa5a3b951a3459fac44b950b178d635-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M160,184.0 L152,172.0 168,172.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-dfa5a3b951a3459fac44b950b178d635-0-2\" stroke-width=\"2px\" d=\"M250,182.0 C250,137.0 305.0,137.0 305.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-dfa5a3b951a3459fac44b950b178d635-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M250,184.0 L242,172.0 258,172.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-dfa5a3b951a3459fac44b950b178d635-0-3\" stroke-width=\"2px\" d=\"M340,182.0 C340,137.0 395.0,137.0 395.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-dfa5a3b951a3459fac44b950b178d635-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M340,184.0 L332,172.0 348,172.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-dfa5a3b951a3459fac44b950b178d635-0-4\" stroke-width=\"2px\" d=\"M430,182.0 C430,137.0 485.0,137.0 485.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-dfa5a3b951a3459fac44b950b178d635-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M430,184.0 L422,172.0 438,172.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-dfa5a3b951a3459fac44b950b178d635-0-5\" stroke-width=\"2px\" d=\"M520,182.0 C520,137.0 575.0,137.0 575.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-dfa5a3b951a3459fac44b950b178d635-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M520,184.0 L512,172.0 528,172.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-dfa5a3b951a3459fac44b950b178d635-0-6\" stroke-width=\"2px\" d=\"M610,182.0 C610,137.0 665.0,137.0 665.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-dfa5a3b951a3459fac44b950b178d635-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M610,184.0 L602,172.0 618,172.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-dfa5a3b951a3459fac44b950b178d635-0-7\" stroke-width=\"2px\" d=\"M790,182.0 C790,137.0 845.0,137.0 845.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-dfa5a3b951a3459fac44b950b178d635-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M790,184.0 L782,172.0 798,172.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-dfa5a3b951a3459fac44b950b178d635-0-8\" stroke-width=\"2px\" d=\"M700,182.0 C700,92.0 850.0,92.0 850.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-dfa5a3b951a3459fac44b950b178d635-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M850.0,184.0 L858.0,172.0 842.0,172.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWOtvT2rHNWy",
        "colab_type": "text"
      },
      "source": [
        "**Write your explanations of the constituency parsing tree and dependency parsing tree here (Question 3-2):** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VA4P1fTWvTB",
        "colab_type": "text"
      },
      "source": [
        "**Answer -**\n",
        "\n",
        "**Constituency Parsing Tree:**\n",
        "\n",
        "\n",
        "*  Constituency parsing aims to extract a constituency-based parse from a sentence that represents its syntactic structure according to a phrase structure grammar.\n",
        "*  By using Constituency parsing i have separated out Nouns and Verbs Phrases from the sentences.\n",
        "\n",
        "\n",
        "*   After separating out Nouns and Verb phrases,further separation is done i.e \"Verbs and Nouns\" are separated from verb phrase.\n",
        "*   I have used nltk i.e. a processing class for deriving trees that represent possible structures for a sequence of tokens. These tree structures are known as “parses”. Typically, parsers are used to derive syntax trees for sentences. \n",
        "\n",
        "**Dependency Parsing Tree:**\n",
        "\n",
        "\n",
        "\n",
        "*   Dependency parsing is the process of extracting the dependency parse of a sentence to represent its grammatical structure. It defines the dependency relationship between headwords and their dependents. The head of a sentence has no dependency and is called the root of the sentence\n",
        "By using dependency parsing i have delineated the dependency between a word (such as a verb) and the phrases which builds upon such as the subject and object phrases of that verb.\n",
        "\n",
        "\n",
        "*   I have used spacy library for dependency parsing.\n",
        "\n",
        "\n",
        "\n",
        "For both the above parser, i have used benepar parser.\n"
      ]
    }
  ]
}