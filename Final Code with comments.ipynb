{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Untitled18.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMNF1MkFIC7NJeb6U1/RW7M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejask666/Tejas_INFO5731_Spring2020/blob/master/Final%20Code%20with%20comments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ov-o489f1_0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium\n",
        "import os\n",
        "import re,datetime\n",
        "import time\n",
        "import csv\n",
        "from selenium.common.exceptions import NoSuchElementException\n",
        "from google.colab import files\n",
        "from selenium import webdriver\n",
        "  \n",
        "class In_Class_Assigment():\n",
        "    def get_data(self,list_of_journal):\n",
        "        list_of_data=[]\n",
        "        print(list_of_journal)\n",
        "        obj = In_Class_Assigment()\n",
        "        # Launch chrome browser.\n",
        "        options = webdriver.ChromeOptions()\n",
        "        # Runs browser in background.\n",
        "        options.add_argument('--headless')\n",
        "        options.add_argument('--no-sandbox')\n",
        "        options.add_argument('--disable-dev-shm-usage')\n",
        "        options.add_argument('--disable-gpu')\n",
        "        options.add_argument(\"--window-size=1920,1080\")\n",
        "        options.add_argument(\"start-maximized\")\n",
        "        options.add_argument(\"disable-infobars\")  \n",
        "        options.add_argument(\"--disable-extensions\")\n",
        "        driver = webdriver.Chrome('chromedriver',options=options)\n",
        "        #This loop will run twice as there are two journals.\n",
        "        for journal in range(len(list_of_journal)):\n",
        "            with open(r'/content/Book1.csv', 'a', newline='') as file:\n",
        "                writer = csv.writer(file)\n",
        "                writer.writerow([list_of_journal[journal][3]])\n",
        "            list_of_data = list_of_journal[journal]\n",
        "            print(list_of_data)\n",
        "            print(list_of_data[2])\n",
        "            #driver.get is use to access url. \n",
        "            driver.get(list_of_data[2])\n",
        "            try:\n",
        "              if driver.find_element_by_xpath(\"(//button[@aria-expanded='false'])[8]\").is_displayed():\n",
        "                # finds year,but before that it checks it is already expanded or not.\n",
        "                 driver.find_element_by_xpath(\"//div[text()='Date Published']\").click()\n",
        "                 time.sleep(5)\n",
        "            except NoSuchElementException as msg:\n",
        "                print(\"No such element found\")\n",
        "            driver.find_element_by_xpath(\"//input[@id='start-year']\").clear()\n",
        "            driver.find_element_by_xpath(\"//input[@id='start-year']\").send_keys(list_of_data[0])\n",
        "            driver.find_element_by_xpath(\"//input[@id='end-year']\").clear()\n",
        "            driver.find_element_by_xpath(\"//input[@id='end-year']\").send_keys(list_of_data[1])\n",
        "            driver.find_element_by_xpath(\"(//input[@id='end-year']//following::input[@type='submit'])[1]\").click()\n",
        "            time.sleep(10)\n",
        "            #It find how many page are there on which articles are present.\n",
        "            total_pages = driver.find_element_by_xpath(\"(//span[@class='number-of-pages'])[1]\").text\n",
        "            count = int(total_pages)\n",
        "            for page in range(count-1):#count\n",
        "                list_of_articles = driver.find_elements_by_xpath(\"//h2//a[contains(@href,'/article/')]\")\n",
        "             # This loop will run for the number pages i.e. on which articles are available.  \n",
        "                for i in range(len(list_of_articles)):  # len(list_of_articles)\n",
        "                    time.sleep(5)\n",
        "                    driver.refresh()\n",
        "                    time.sleep(5)\n",
        "                    list_of_articles = driver.find_elements_by_xpath(\"//h2//a[contains(@href,'/article/')]\")\n",
        "            # This is use to scroll the page.\n",
        "                    driver.execute_script(\"arguments[0].scrollIntoView();\", list_of_articles[i])\n",
        "                    list_of_articles[i].click()\n",
        "            #This finds webelement i.e.location of references        \n",
        "                    list_of_ref = driver.find_elements_by_xpath(\"//p[contains(@class,'c-article-references__text')]\")\n",
        "                    if list_of_ref != []:\n",
        "                        for j in range(len(list_of_ref)):\n",
        "            # Actual content i.e. data which we want to extract from webpage is saved in reference_text in            \n",
        "                            reference_text = list_of_ref[j].text\n",
        "                            author = obj.get_author(reference_text)\n",
        "                            if author is None:\n",
        "                                continue\n",
        "                            author.append(list_of_data[3])\n",
        "                            obj.writeintocsv(author)\n",
        "                        driver.back()\n",
        "                    else:\n",
        "                        driver.back()\n",
        "                button=driver.find_element_by_xpath(\"(//img[contains(@src,'/images/arrow-right.png')])[1]\")\n",
        "                driver.execute_script(\"arguments[0].scrollIntoView();\", button)     \n",
        "                button.click()\n",
        "                time.sleep(10)\n",
        "\n",
        "    def get_author(self, reference):\n",
        "\n",
        "           author_detail=[]\n",
        "           tapp=[]\n",
        "           tapp[:0]=reference\n",
        "           #Separates author name and year, as year is mentioned in () with sentences and extracting year.  \n",
        "           remove=\"1234567890\"\n",
        "           temp=\"\"\n",
        "           for i in range(len(tapp)):\n",
        "              if tapp[i] in remove:\n",
        "                 break\n",
        "              else:\n",
        "                temp=temp+tapp[i]\n",
        "           author_name=temp.strip(\".(\")\n",
        "           try:\n",
        "             # Finding 4 digit in reference \n",
        "              match= re.search(r'\\d{4}',reference)\n",
        "              #Extracting year\n",
        "              date = datetime.datetime.strptime(match.group(), '%Y').date()\n",
        "              year=date.year\n",
        "           except:\n",
        "             author_detail=None\n",
        "             return author_detail\n",
        "             #Spliting article and year as year is always after dot\n",
        "           article_title=reference.split(\").\")\n",
        "           if len(article_title)>2:\n",
        "              try:\n",
        "                title=re.search(\"(?P<url>https?://[^\\s]+)\", article_title[2]).group(\"url\")\n",
        "              except:\n",
        "                list_of_sentence=article_title[2].split(\".\")\n",
        "                title=list_of_sentence[0].strip(\" \")\n",
        "           else:\n",
        "              if len(article_title)==1:\n",
        "                try:\n",
        "                  title=re.search(\"(?P<url>https?://[^\\s]+)\", article_title[2]).group(\"url\")\n",
        "                except:\n",
        "                   title=article_title[0]\n",
        "              else:\n",
        "                list_of_sen=article_title[1].split(\".\")\n",
        "                title=list_of_sen[0].strip(\" \")    \n",
        "                #checking if author or year or author name any of the detail is not available it will none the author details\n",
        "           if title==\"\" or year==\"\" or author_name==\"\":\n",
        "                author_detail=None\n",
        "                return author_detail\n",
        "           else:\n",
        "                author_detail.append(title)\n",
        "                author_detail.append(year)\n",
        "                author_detail.append(author_name)\n",
        "                return author_detail\n",
        "                \n",
        "               #writing all information in csv file\n",
        "    def writeintocsv(self,ref_details):\n",
        "        with open(r'/content/Book1.csv', 'a', newline='') as file:\n",
        "                writer = csv.writer(file)\n",
        "                if len(ref_details)<=2:\n",
        "                   pass\n",
        "                else:\n",
        "                   writer.writerow([\"\",ref_details[0],ref_details[1],str(ref_details[2]).strip('[]')])\n",
        "#This checks file already exists or not on path\n",
        "object = In_Class_Assigment()\n",
        "isExist = os.path.exists('/content/Book1.csv')\n",
        "if isExist:\n",
        "         os.remove('/content/Book1.csv')\n",
        "#  if file already exists it will append in that otherwise it will create new file and append data in it.         \n",
        "with open(r'/content/Book1.csv', 'a+', newline='') as file:\n",
        "# Object created to use csv.writer applications.  \n",
        "                writer = csv.writer(file)\n",
        "                writer.writerow([\"Journal Names\",\"Article Title\",\"Year\",\"Author\"]) \n",
        "data_list=[(\"1937\", \"2019\",\"https://link.springer.com/search?query=&search-within=Journal&facet-journal-id=40732\",\"Psychological Record\")]\n",
        "object.get_data(data_list)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}