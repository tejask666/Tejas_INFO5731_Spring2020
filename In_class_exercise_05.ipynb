{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of In_class_exercise_05.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejask666/Tejas_INFO5731_Spring2020/blob/master/In_class_exercise_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7TahL04sVvR",
        "colab_type": "text"
      },
      "source": [
        "# **The fifth in-class-exercise**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejyZITr8sjnh",
        "colab_type": "text"
      },
      "source": [
        "## **1. Rule-based information extraction**\n",
        "\n",
        "Use any keywords related to data science, natural language processing, machine learning to search from google scholar, get the **titles** of 100 articles (either by web scraping or manually) about this topic, define a set of patterns to extract the research questions/problems, methods/algorithms/models, datasets, applications, or any other important information about this topic. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJB19NlKx6i_",
        "colab_type": "code",
        "outputId": "28c30182-1b5b-4165-b7b3-9b5164649493",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# I have used below code to get 100 titles--but same code was working in pycharm but it is not working in colab so after getting 100 titles i made list and had set pattern for that\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium\n",
        "import nltk\n",
        "import time\n",
        "from selenium import webdriver\n",
        "\n",
        "class In_Class_5():\n",
        "\n",
        "\n",
        "      def launchgoogle(self):\n",
        "             nltk.download('punkt')\n",
        "             nltk.download('wordnet')\n",
        "             nltk.download('averaged_perceptron_tagger')\n",
        "             options = webdriver.ChromeOptions()\n",
        "             options.add_argument('--headless')\n",
        "             options.add_argument('--no-sandbox')\n",
        "             options.add_argument('--disable-dev-shm-usage')\n",
        "             driver = webdriver.Chrome('chromedriver',options=options)\n",
        "             driver.get(\"https://scholar.google.com/\")\n",
        "             obj=In_Class_5()\n",
        "             keyword_list=[\"artificial intelligence\",\"natural language\"]\n",
        "             result=obj.get_data(keyword_list, driver)\n",
        "             print(result)\n",
        "        \n",
        "\n",
        "      def get_data(self,search_keyword,driver):  \n",
        "             result_list=[]\n",
        "             time.sleep(10)\n",
        "             for i in range(len(search_keyword)):\n",
        "                driver.find_element_by_xpath(\"//form//div//input[@aria-label='Search']\").clear()\n",
        "                driver.find_element_by_xpath(\"//form//div//input[@aria-label='Search']\").send_keys(search_keyword[i])\n",
        "                action = ActionChains(driver)\n",
        "                action.send_keys(Keys.ENTER).perform()\n",
        "                google_list = driver.find_elements_by_xpath(\"//input[@aria-label='Search']//following::h3\")\n",
        "                for i in range(len(google_list)):\n",
        "                  result_list.append(google_list[i].text)\n",
        "             return result_list          \n",
        "   \n",
        "\n",
        "a=In_Class_5()\n",
        "a.launchgoogle()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rIgn:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.173)] [Connecting to security.u\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.173)] [Connecting to security.u\r                                                                               \rGet:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [564 B]\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [3 Release\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Waiting f\r                                                                               \rGet:4 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "\r0% [Waiting for headers] [Waiting for headers] [4 InRelease 0 B/3,626 B 0%] [Wa\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Waiting f\r                                                                               \rHit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "\r                                                                               \r0% [Waiting for headers] [Waiting for headers] [Waiting for headers]\r0% [4 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rGet:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [819 B]\n",
            "\r0% [4 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [6 Re\r0% [4 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rHit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\r0% [4 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rGet:8 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r0% [4 InRelease gpgv 3,626 B] [Waiting for headers] [8 InRelease 14.2 kB/88.7 k\r                                                                               \rGet:9 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "\r0% [4 InRelease gpgv 3,626 B] [Waiting for headers] [8 InRelease 14.2 kB/88.7 k\r0% [Waiting for headers] [8 InRelease 14.2 kB/88.7 kB 16%] [9 InRelease 14.2 kB\r0% [Release.gpg gpgv 564 B] [Waiting for headers] [8 InRelease 14.2 kB/88.7 kB \r                                                                               \rGet:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "\r0% [Release.gpg gpgv 564 B] [10 InRelease 14.2 kB/88.7 kB 16%] [8 InRelease 17.\r0% [Release.gpg gpgv 564 B] [10 InRelease 15.6 kB/88.7 kB 18%] [8 InRelease 22.\r0% [10 InRelease 15.6 kB/88.7 kB 18%] [8 InRelease 43.1 kB/88.7 kB 49%] [Waitin\r0% [6 Release.gpg gpgv 564 B] [10 InRelease 15.6 kB/88.7 kB 18%] [8 InRelease 4\r                                                                               \rGet:12 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ Packages [87.3 kB]\n",
            "\r0% [6 Release.gpg gpgv 564 B] [10 InRelease 33.0 kB/88.7 kB 37%] [8 InRelease 6\r0% [6 Release.gpg gpgv 564 B] [10 InRelease 41.7 kB/88.7 kB 47%] [8 InRelease 6\r0% [12 Packages store 0 B] [6 Release.gpg gpgv 564 B] [10 InRelease 41.7 kB/88.\r0% [6 Release.gpg gpgv 564 B] [10 InRelease 47.5 kB/88.7 kB 54%] [8 InRelease 7\r0% [6 Release.gpg gpgv 564 B] [10 InRelease 47.5 kB/88.7 kB 54%] [Waiting for h\r                                                                               \r0% [10 InRelease 59.1 kB/88.7 kB 67%] [Waiting for headers]\r                                                           \rGet:13 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "\r0% [10 InRelease 59.1 kB/88.7 kB 67%] [13 InRelease 14.2 kB/15.4 kB 92%]\r0% [7 InRelease gpgv 242 kB] [10 InRelease 59.1 kB/88.7 kB 67%] [Waiting for he\r0% [7 InRelease gpgv 242 kB] [10 InRelease 62.0 kB/88.7 kB 70%] [Waiting for he\r                                                                               \rGet:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [141 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:16 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [37.1 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [826 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,128 kB]\n",
            "Get:19 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,784 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [836 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [7,640 B]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [11.7 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,355 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [4,245 B]\n",
            "Get:25 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [861 kB]\n",
            "Fetched 7,374 kB in 4s (1,779 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension adobe-flashplugin\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 90 not upgraded.\n",
            "Need to get 74.4 MB of archives.\n",
            "After this operation, 264 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 80.0.3987.87-0ubuntu0.18.04.1 [1,095 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 80.0.3987.87-0ubuntu0.18.04.1 [66.1 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 80.0.3987.87-0ubuntu0.18.04.1 [3,155 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 80.0.3987.87-0ubuntu0.18.04.1 [4,044 kB]\n",
            "Fetched 74.4 MB in 5s (15.3 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 134448 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_80.0.3987.87-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (80.0.3987.87-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_80.0.3987.87-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (80.0.3987.87-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_80.0.3987.87-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (80.0.3987.87-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_80.0.3987.87-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (80.0.3987.87-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (80.0.3987.87-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (80.0.3987.87-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (80.0.3987.87-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (80.0.3987.87-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n",
            "Collecting selenium\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K     |████████████████████████████████| 911kB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\n",
            "Installing collected packages: selenium\n",
            "Successfully installed selenium-3.141.0\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-2dfb85bf451f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIn_Class_5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunchgoogle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-2dfb85bf451f>\u001b[0m in \u001b[0;36mlaunchgoogle\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     22\u001b[0m              \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIn_Class_5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m              \u001b[0mkeyword_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"artificial intelligence\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"natural language\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m              \u001b[0mresult\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyword_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m              \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-2dfb85bf451f>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, search_keyword, driver)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"//form//div//input[@aria-label='Search']\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"//form//div//input[@aria-label='Search']\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_keyword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActionChains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                 \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENTER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mgoogle_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_elements_by_xpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"//input[@aria-label='Search']//following::h3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ActionChains' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvR_O9D8sOUY",
        "colab_type": "code",
        "outputId": "93f4e746-2368-4731-b197-fc792fa5dbd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "# Write your code here\n",
        "import re \n",
        "import string \n",
        "import nltk \n",
        "import spacy \n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import math \n",
        "from tqdm import tqdm \n",
        "from spacy.matcher import Matcher \n",
        "from spacy.tokens import Span \n",
        "from spacy import displacy \n",
        "\n",
        "class In_Class_5(): \n",
        "\n",
        "  def get_data(self,list_of_titles,pattern): \n",
        "    result=[]\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    for i in range(len(list_of_titles)):\n",
        "      doc = nlp(list_of_titles[i])\n",
        "      matcher = Matcher(nlp.vocab) \n",
        "      matcher.add(\"matching_1\", None, pattern) \n",
        "      matches = matcher(doc) \n",
        "      for match_id, start, end in matches:\n",
        "        span = doc[start:end]\n",
        "        if span.text != \"\":\n",
        "          result.append(list_of_titles[i])\n",
        "    return result  \n",
        "\n",
        "comparision = [{'POS':'PROPN'}, \n",
        "               {'LOWER': 'vs.'}, \n",
        "               {'POS': 'PROPN','OP':'?'}, \n",
        "              ]\n",
        "\n",
        "reports = [{'POS':'NOUN'}, \n",
        "           {'POS': 'NOUN'}, \n",
        "           {'LOWER': 'report'} \n",
        "            ]\n",
        "\n",
        "important_topics = [\n",
        "              {'LOWER': 'data'},\n",
        "              {'POS': 'NOUN'}, \n",
        "              ]\n",
        "\n",
        "model=     [ {'POS': 'NOUN', 'OP':'*'},\n",
        "             {'POS': 'PROPN', 'OP':'*'},\n",
        "             {'lower': 'model'}\n",
        "           ]\n",
        "\n",
        "questions=[ \n",
        "           {'POS': 'PROPN','OP':'*'},\n",
        "           {'IS_PUNCT': True, 'OP': '?'}\n",
        "           ]\n",
        "title_list= [\"Bayesian artificial intelligence\",\"Support Vector Machines for Machine Learning\",\"Understanding the Mathematics behind Support Vector Machines\",\"Lexalytics: Data Analytics with NLP & Text Analytics\",\"Scholarly articles for Support Vector Machines.\",\"Tokenization in NLP! - Intellipaat Community\",\"Benchmarking Python NLP Tokenizers - Towards Data Science\",\"Data Science vs. Machine Learning\",\"Principles of artificial intelligence\",\"Artificial intelligence: a modern approach\",\"The handbook of artificial intelligence\",\"Prolog programming for artificial intelligence\",\"Introduction to artificial intelligence\",\"Artificial intelligence: a new synthesis\",\"An artificial intelligence approach\",\"Distributed artificial intelligence\",\"Logical foundations of artificial intelligence\",\"Artificial Intelligence vs. Internet Of Things\",\"What is the difference between supervised and unsupervised machine learning?\",\"Foundations of statistical natural language processing\",\"Natural language processing from scratch\",\"The Stanford CoreNLP natural language processing toolkit\",\"A maximum entropy approach to natural language processing\",\"Natural language processing with Python: analyzing text with the natural language toolkit\",\"Natural language processing\",\"Introduction to Arabic natural language processing\",\"Natural language processing\",\"Sentiment analysis: Capturing favorability using natural language processing\",\"Readings in natural language processing\",\"Data science in action\",\"Data science and prediction\",\"Data science and its relationship to big data and data-driven decision making\",\"Data Science for Business: What you need to know about data mining and data-analytic thinking\",\"predictive analytics and big data: a revolution that will transform supply chain design and management\",\"The quantified self: Fundamental disruption in big data science and biological discovery\",\"The fourth paradigm: data-intensive scientific discovery\",\"Big data and science: Myths and reality\",\"Analysis of symbolic data: exploratory methods for extracting statistical information from complex data\",\"Big data: astronomical or genomical?\",\"Principles of data mining\",\"Privacy-preserving data mining\",\"Data mining: concepts and techniques\",\"Data preparation for data mining\",\"Untangling text data mining\",\"Data mining applications in healthcare\",\"Relational data mining\",\"Data warehousing\",\"data mining and OLAP\",\"Data mining with big data\",\"Discovering knowledge in data: an introduction to data mining\",\"Unsupervised large-vocabulary word sense disambiguation with graph-based algorithms for sequence data labeling\",\"Data labeling technique for high performance protocol processing on data labeling for clustering categorical data\",\"Clamshell: Speeding up crowds for low-latency data labeling\",\"Multi-level and multi-category data labeling system\",\"A data labeling method for clustering categorical data\",\"Language context dependent data labeling\",\"Labeling unclustered categorical data into clusters based on the important attribute values\",\"Interactive image data labeling using self-organizing maps in an augmented reality scenario\",\"Estimating CT image from MRI data using 3D fully convolutional networks\",\"A method of computing generalized Bayesian probability values for expert systems\",\"Landslide susceptibility analysis and verification using the Bayesian probability model\",\"Classical confidence intervals and Bayesian probability estimates for ends of local taxon ranges\",\"[HTML] Bootstrap\",\"Bayesian probability and maximum likelihood mapping: exploring new tools for comparative genome analyses\",\"On confidence points and Bayesian probability points in the case of several parameters\",\"Application of Bayesian probability network to music scene analysis\",\"[HTML] Confidence as Bayesian probability: From neural origins to behavior\",\"Assessing the decline of brown trout (Salmo trutta) in Swiss rivers using a Bayesian probability network\",\"An introduction to parameter estimation using Bayesian probability theory\",\"Obtaining calibrated probability estimates from decision trees and naive Bayesian classifiers\",\"Rules of thumb in data engineering\",\"Modeling and simulation-based data engineering: introducing pragmatics into ontologies for net-centric information exchange\",\"Data quality requirements analysis and modeling\",\"Engineering aspects of shape memory alloys\",\"Learning from imbalanced data\",\"The management of probabilistic data\",\"The New Jersey data reduction report\",\"Hive-a petabyte scale data warehouse using hadoop\",\"Efficient storage of XML data\",\"PeerDB: A P2P-based system for distributed data sharing\",\"The Future of Data Science, Machine Learning and AI - Gartner\",\"Statistics and Artificial Intelligence for Data Science\",\"Introduction to machine learning\",\"Machine learning: a probabilistic perspective\",\"Machine learning: an algorithmic perspective\",\"Foundations of machine learning\",\"Genetic algorithms and machine learning\",\"C4. 5: programs for machine learning\",\"Machine-learning research\",\"Pattern recognition and machine learning\",\"SOM-based data visualization methods\",\"Lattice: multivariate data visualization with R\",\"Visual cues: practical data visualization\",\"Interactive high-dimensional data visualization\",\"Integrative Genomics Viewer (IGV): high-performance genomics data visualization and exploration\",\"Handbook of data visualization\",\"Paraview: An end-user tool for large data visualization\",\"Interactive data visualization: foundations,techniques and applications\",\"Spot noise texture synthesis for data visualization\",\"Choosing effective colours for data visualization\"]\n",
        "obj=In_Class_5()\n",
        "print(\"list of reports: \"+str(obj.get_data(title_list,reports)))\n",
        "print(\"list of important topics: \"+str(obj.get_data(title_list,important_topics)))\n",
        "print(\"list of model: \"+str(obj.get_data(title_list,model)))\n",
        "print(\"list of questions: \"+str(obj.get_data(title_list,questions)))\n",
        "print(\"list of comparision titles: \"+str(obj.get_data(title_list,comparision)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "list of reports: ['The New Jersey data reduction report']\n",
            "list of important topics: ['Data science in action', 'Data science and prediction', 'Data science and its relationship to big data and data-driven decision making', 'Data Science for Business: What you need to know about data mining and data-analytic thinking', 'The quantified self: Fundamental disruption in big data science and biological discovery', 'Principles of data mining', 'Privacy-preserving data mining', 'Data mining: concepts and techniques', 'Data preparation for data mining', 'Data preparation for data mining', 'Untangling text data mining', 'Data mining applications in healthcare', 'Relational data mining', 'Data warehousing', 'data mining and OLAP', 'Data mining with big data', 'Discovering knowledge in data: an introduction to data mining', 'Unsupervised large-vocabulary word sense disambiguation with graph-based algorithms for sequence data labeling', 'Data labeling technique for high performance protocol processing on data labeling for clustering categorical data', 'Data labeling technique for high performance protocol processing on data labeling for clustering categorical data', 'Clamshell: Speeding up crowds for low-latency data labeling', 'Multi-level and multi-category data labeling system', 'A data labeling method for clustering categorical data', 'Language context dependent data labeling', 'Interactive image data labeling using self-organizing maps in an augmented reality scenario', 'Rules of thumb in data engineering', 'Modeling and simulation-based data engineering: introducing pragmatics into ontologies for net-centric information exchange', 'Data quality requirements analysis and modeling', 'The New Jersey data reduction report', 'Hive-a petabyte scale data warehouse using hadoop', 'PeerDB: A P2P-based system for distributed data sharing', 'SOM-based data visualization methods', 'Lattice: multivariate data visualization with R', 'Visual cues: practical data visualization', 'Interactive high-dimensional data visualization', 'Integrative Genomics Viewer (IGV): high-performance genomics data visualization and exploration', 'Handbook of data visualization', 'Paraview: An end-user tool for large data visualization', 'Interactive data visualization: foundations,techniques and applications', 'Spot noise texture synthesis for data visualization', 'Choosing effective colours for data visualization']\n",
            "list of model: ['Landslide susceptibility analysis and verification using the Bayesian probability model', 'Landslide susceptibility analysis and verification using the Bayesian probability model']\n",
            "list of questions: ['Support Vector Machines for Machine Learning', 'Support Vector Machines for Machine Learning', 'Support Vector Machines for Machine Learning', 'Support Vector Machines for Machine Learning', 'Support Vector Machines for Machine Learning', 'Support Vector Machines for Machine Learning', 'Support Vector Machines for Machine Learning', 'Support Vector Machines for Machine Learning', 'Support Vector Machines for Machine Learning', 'Understanding the Mathematics behind Support Vector Machines', 'Understanding the Mathematics behind Support Vector Machines', 'Understanding the Mathematics behind Support Vector Machines', 'Understanding the Mathematics behind Support Vector Machines', 'Understanding the Mathematics behind Support Vector Machines', 'Understanding the Mathematics behind Support Vector Machines', 'Understanding the Mathematics behind Support Vector Machines', 'Lexalytics: Data Analytics with NLP & Text Analytics', 'Lexalytics: Data Analytics with NLP & Text Analytics', 'Lexalytics: Data Analytics with NLP & Text Analytics', 'Lexalytics: Data Analytics with NLP & Text Analytics', 'Lexalytics: Data Analytics with NLP & Text Analytics', 'Lexalytics: Data Analytics with NLP & Text Analytics', 'Lexalytics: Data Analytics with NLP & Text Analytics', 'Lexalytics: Data Analytics with NLP & Text Analytics', 'Lexalytics: Data Analytics with NLP & Text Analytics', 'Scholarly articles for Support Vector Machines.', 'Scholarly articles for Support Vector Machines.', 'Scholarly articles for Support Vector Machines.', 'Scholarly articles for Support Vector Machines.', 'Scholarly articles for Support Vector Machines.', 'Scholarly articles for Support Vector Machines.', 'Scholarly articles for Support Vector Machines.', 'Tokenization in NLP! - Intellipaat Community', 'Tokenization in NLP! - Intellipaat Community', 'Tokenization in NLP! - Intellipaat Community', 'Tokenization in NLP! - Intellipaat Community', 'Tokenization in NLP! - Intellipaat Community', 'Tokenization in NLP! - Intellipaat Community', 'Benchmarking Python NLP Tokenizers - Towards Data Science', 'Benchmarking Python NLP Tokenizers - Towards Data Science', 'Benchmarking Python NLP Tokenizers - Towards Data Science', 'Benchmarking Python NLP Tokenizers - Towards Data Science', 'Benchmarking Python NLP Tokenizers - Towards Data Science', 'Benchmarking Python NLP Tokenizers - Towards Data Science', 'Benchmarking Python NLP Tokenizers - Towards Data Science', 'Benchmarking Python NLP Tokenizers - Towards Data Science', 'Benchmarking Python NLP Tokenizers - Towards Data Science', 'Benchmarking Python NLP Tokenizers - Towards Data Science', 'Benchmarking Python NLP Tokenizers - Towards Data Science', 'Benchmarking Python NLP Tokenizers - Towards Data Science', 'Benchmarking Python NLP Tokenizers - Towards Data Science', 'Data Science vs. Machine Learning', 'Data Science vs. Machine Learning', 'Data Science vs. Machine Learning', 'Data Science vs. Machine Learning', 'Data Science vs. Machine Learning', 'Data Science vs. Machine Learning', 'Artificial intelligence: a modern approach', 'Prolog programming for artificial intelligence', 'Artificial intelligence: a new synthesis', 'Artificial Intelligence vs. Internet Of Things', 'Artificial Intelligence vs. Internet Of Things', 'Artificial Intelligence vs. Internet Of Things', 'What is the difference between supervised and unsupervised machine learning?', 'The Stanford CoreNLP natural language processing toolkit', 'Natural language processing with Python: analyzing text with the natural language toolkit', 'Natural language processing with Python: analyzing text with the natural language toolkit', 'Sentiment analysis: Capturing favorability using natural language processing', 'Data science and its relationship to big data and data-driven decision making', 'Data Science for Business: What you need to know about data mining and data-analytic thinking', 'Data Science for Business: What you need to know about data mining and data-analytic thinking', 'Data Science for Business: What you need to know about data mining and data-analytic thinking', 'Data Science for Business: What you need to know about data mining and data-analytic thinking', 'Data Science for Business: What you need to know about data mining and data-analytic thinking', 'predictive analytics and big data: a revolution that will transform supply chain design and management', 'The quantified self: Fundamental disruption in big data science and biological discovery', 'The fourth paradigm: data-intensive scientific discovery', 'The fourth paradigm: data-intensive scientific discovery', 'Big data and science: Myths and reality', 'Analysis of symbolic data: exploratory methods for extracting statistical information from complex data', 'Big data: astronomical or genomical?', 'Big data: astronomical or genomical?', 'Privacy-preserving data mining', 'Data mining: concepts and techniques', 'data mining and OLAP', 'Discovering knowledge in data: an introduction to data mining', 'Unsupervised large-vocabulary word sense disambiguation with graph-based algorithms for sequence data labeling', 'Unsupervised large-vocabulary word sense disambiguation with graph-based algorithms for sequence data labeling', 'Clamshell: Speeding up crowds for low-latency data labeling', 'Clamshell: Speeding up crowds for low-latency data labeling', 'Clamshell: Speeding up crowds for low-latency data labeling', 'Multi-level and multi-category data labeling system', 'Multi-level and multi-category data labeling system', 'Interactive image data labeling using self-organizing maps in an augmented reality scenario', 'Estimating CT image from MRI data using 3D fully convolutional networks', '[HTML] Bootstrap', '[HTML] Bootstrap', '[HTML] Bootstrap', 'Bayesian probability and maximum likelihood mapping: exploring new tools for comparative genome analyses', '[HTML] Confidence as Bayesian probability: From neural origins to behavior', '[HTML] Confidence as Bayesian probability: From neural origins to behavior', '[HTML] Confidence as Bayesian probability: From neural origins to behavior', 'Assessing the decline of brown trout (Salmo trutta) in Swiss rivers using a Bayesian probability network', 'Assessing the decline of brown trout (Salmo trutta) in Swiss rivers using a Bayesian probability network', 'Assessing the decline of brown trout (Salmo trutta) in Swiss rivers using a Bayesian probability network', 'Modeling and simulation-based data engineering: introducing pragmatics into ontologies for net-centric information exchange', 'Modeling and simulation-based data engineering: introducing pragmatics into ontologies for net-centric information exchange', 'Modeling and simulation-based data engineering: introducing pragmatics into ontologies for net-centric information exchange', 'The New Jersey data reduction report', 'The New Jersey data reduction report', 'The New Jersey data reduction report', 'Hive-a petabyte scale data warehouse using hadoop', 'Hive-a petabyte scale data warehouse using hadoop', 'PeerDB: A P2P-based system for distributed data sharing', 'PeerDB: A P2P-based system for distributed data sharing', 'PeerDB: A P2P-based system for distributed data sharing', 'The Future of Data Science, Machine Learning and AI - Gartner', 'The Future of Data Science, Machine Learning and AI - Gartner', 'The Future of Data Science, Machine Learning and AI - Gartner', 'The Future of Data Science, Machine Learning and AI - Gartner', 'The Future of Data Science, Machine Learning and AI - Gartner', 'The Future of Data Science, Machine Learning and AI - Gartner', 'The Future of Data Science, Machine Learning and AI - Gartner', 'The Future of Data Science, Machine Learning and AI - Gartner', 'The Future of Data Science, Machine Learning and AI - Gartner', 'The Future of Data Science, Machine Learning and AI - Gartner', 'The Future of Data Science, Machine Learning and AI - Gartner', 'Statistics and Artificial Intelligence for Data Science', 'Statistics and Artificial Intelligence for Data Science', 'Statistics and Artificial Intelligence for Data Science', 'Statistics and Artificial Intelligence for Data Science', 'Statistics and Artificial Intelligence for Data Science', 'Statistics and Artificial Intelligence for Data Science', 'Machine learning: a probabilistic perspective', 'Machine learning: an algorithmic perspective', 'C4. 5: programs for machine learning', 'C4. 5: programs for machine learning', 'Machine-learning research', 'SOM-based data visualization methods', 'SOM-based data visualization methods', 'Lattice: multivariate data visualization with R', 'Visual cues: practical data visualization', 'Interactive high-dimensional data visualization', 'Integrative Genomics Viewer (IGV): high-performance genomics data visualization and exploration', 'Integrative Genomics Viewer (IGV): high-performance genomics data visualization and exploration', 'Integrative Genomics Viewer (IGV): high-performance genomics data visualization and exploration', 'Integrative Genomics Viewer (IGV): high-performance genomics data visualization and exploration', 'Integrative Genomics Viewer (IGV): high-performance genomics data visualization and exploration', 'Integrative Genomics Viewer (IGV): high-performance genomics data visualization and exploration', 'Integrative Genomics Viewer (IGV): high-performance genomics data visualization and exploration', 'Integrative Genomics Viewer (IGV): high-performance genomics data visualization and exploration', 'Integrative Genomics Viewer (IGV): high-performance genomics data visualization and exploration', 'Integrative Genomics Viewer (IGV): high-performance genomics data visualization and exploration', 'Integrative Genomics Viewer (IGV): high-performance genomics data visualization and exploration', 'Handbook of data visualization', 'Paraview: An end-user tool for large data visualization', 'Paraview: An end-user tool for large data visualization', 'Paraview: An end-user tool for large data visualization', 'Interactive data visualization: foundations,techniques and applications', 'Interactive data visualization: foundations,techniques and applications']\n",
            "list of comparision titles: ['Data Science vs. Machine Learning', 'Artificial Intelligence vs. Internet Of Things']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq_7VGmrsum4",
        "colab_type": "text"
      },
      "source": [
        "## **2. Domain-specific information extraction**\n",
        "\n",
        "For the legal case used in the data cleaning exercise: [01-05-1 Adams v Tanner.txt](https://raw.githubusercontent.com/unt-iialab/INFO5731_Spring2020/master/In_class_exercise/01-05-1%20%20Adams%20v%20Tanner.txt), use [legalNLP](https://lexpredict-lexnlp.readthedocs.io/en/latest/modules/extract/extract.html#nlp-based-extraction-methods) to extract the following inforation from the text (if the information is not exist, just print None):\n",
        "\n",
        "(1) acts, e.g., “section 1 of the Advancing Hope Act, 1986”\n",
        "\n",
        "(2) amounts, e.g., “ten pounds” or “5.8 megawatts”\n",
        "\n",
        "(3) citations, e.g., “10 U.S. 100” or “1998 S. Ct. 1”\n",
        "\n",
        "(4) companies, e.g., “Lexpredict LLC”\n",
        "\n",
        "(5) conditions, e.g., “subject to …” or “unless and until …”\n",
        "\n",
        "(6) constraints, e.g., “no more than”\n",
        "\n",
        "(7) copyright, e.g., “(C) Copyright 2000 Acme”\n",
        "\n",
        "(8) courts, e.g., “Supreme Court of New York”\n",
        "\n",
        "(9) CUSIP, e.g., “392690QT3”\n",
        "\n",
        "(10) dates, e.g., “June 1, 2017” or “2018-01-01”\n",
        "\n",
        "(11) definitions, e.g., “Term shall mean …”\n",
        "\n",
        "(12) distances, e.g., “fifteen miles”\n",
        "\n",
        "(13) durations, e.g., “ten years” or “thirty days”\n",
        "\n",
        "(14) geographic and geopolitical entities, e.g., “New York” or “Norway”\n",
        "\n",
        "(15) money and currency usages, e.g., “$5” or “10 Euro”\n",
        "\n",
        "(16) percents and rates, e.g., “10%” or “50 bps”\n",
        "\n",
        "(17) PII, e.g., “212-212-2121” or “999-999-9999”\n",
        "\n",
        "(18) ratios, e.g.,” 3:1” or “four to three”\n",
        "\n",
        "(19) regulations, e.g., “32 CFR 170”\n",
        "\n",
        "(20) trademarks, e.g., “MyApp (TM)”\n",
        "\n",
        "(21) URLs, e.g., “http://acme.com/”\n",
        "\n",
        "(22) addresses, e.g., “1999 Mount Read Blvd, Rochester, NY, USA, 14615”\n",
        "\n",
        "(23) persons, e.g., “John Doe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc7NtJrLx5tS",
        "colab_type": "code",
        "outputId": "5043142c-09cd-49bf-b09d-a7ed94183a6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 970
        }
      },
      "source": [
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium\n",
        "import lexnlp\n",
        "import spacy        \n",
        "import string\n",
        "import pandas\n",
        "import nltk\n",
        "import nltk\n",
        "from nltk.tag.stanford import StanfordNERTagger\n",
        "from selenium import webdriver\n",
        "\n",
        "class In_Class_5():\n",
        "\n",
        "\n",
        "      def get_data(self):\n",
        "             nltk.download('punkt')\n",
        "             nltk.download('wordnet')\n",
        "             nltk.download('averaged_perceptron_tagger')\n",
        "             options = webdriver.ChromeOptions()\n",
        "             options.add_argument('--headless')\n",
        "             options.add_argument('--no-sandbox')\n",
        "             options.add_argument('--disable-dev-shm-usage')\n",
        "             driver = webdriver.Chrome('chromedriver',options=options)\n",
        "             driver.get(\"https://raw.githubusercontent.com/unt-iialab/INFO5731_Spring2020/master/In_class_exercise/01-05-1%20%20Adams%20v%20Tanner.txt\")\n",
        "             content=driver.find_element_by_tag_name('pre').text\n",
        "             obj=In_Class_5()\n",
        "             obj.print_data((list(lexnlp.extract.en.amounts.get_amounts(content))),\"amounts\")\n",
        "             obj.print_data((list(lexnlp.extract.en.acts.get_act_list(content))),\"acts\")\n",
        "             obj.print_data((list(lexnlp.extract.en.citations.get_citations(content))),\"citation\")\n",
        "             obj.print_data((list(lexnlp.extract.en.entities.nltk_re.get_companies(content))),\"companies\")\n",
        "             obj.print_data((list(lexnlp.extract.en.conditions.get_conditions(content))),\"conditions\")\n",
        "             obj.print_data((list(lexnlp.extract.en.constraints.get_constraints(content))),\"constraints\")\n",
        "             obj.print_data((list(lexnlp.extract.en.copyright.get_copyright(content))),\"copyright\")\n",
        "             court = pandas.read_csv(\"https://raw.githubusercontent.com/LexPredict/lexpredict-legal-dictionary/1.0.5/en/legal/us_courts.csv\")\n",
        "             court_config = []\n",
        "             for _, row in court.iterrows():\n",
        "                c = lexnlp.extract.en.dict_entities.entity_config(row[\"Court ID\"], row[\"Court Name\"], 0, row[\"Alias\"].split(\";\") if not pandas.isnull(row[\"Alias\"]) else [])\n",
        "                court_config.append(c)\n",
        "             for entity, alias in lexnlp.extract.en.courts.get_courts(content, court_config):\n",
        "                  print('list of courts is:')\n",
        "                  print(\"entity=\", entity)\n",
        "             geo= pandas.read_csv(\"https://raw.githubusercontent.com/LexPredict/lexpredict-lexnlp/master/test_data/lexnlp/extract/en/tests/test_geoentities/geoentities.csv\")\n",
        "             geo_config = []\n",
        "             for _, row in geo.iterrows():\n",
        "              c = lexnlp.extract.en.dict_entities.entity_config(row[\"entity_id\"], row[\"name\"],row[\"category\"])\n",
        "              geo_config.append(c)\n",
        "             print('list of geographical entities is:')\n",
        "             for entity, alias in lexnlp.extract.en.geoentities.get_geoentities(content,geo_config):\n",
        "              print(\"entity=\", entity)\n",
        "             obj.print_data((list(lexnlp.extract.en.cusip.get_cusip(content))),\"cusip\")\n",
        "             obj.print_data((list(lexnlp.extract.en.dates.get_dates(content))),\"dates\")\n",
        "             obj.print_data((list(lexnlp.extract.en.definitions.get_definitions(content))),\"definitions\")\n",
        "             obj.print_data((list(lexnlp.extract.en.distances.get_distances(content))),\"distances\")\n",
        "             obj.print_data((list(lexnlp.extract.en.durations.get_durations(content))),\"durations\")\n",
        "             obj.print_data((list(lexnlp.extract.en.money.get_money(content))),\"money\")\n",
        "             obj.print_data((list(lexnlp.extract.en.percents.get_percents(content))),\"percents\")\n",
        "             obj.print_data((list(lexnlp.extract.en.pii.get_pii(content))),\"PII\")\n",
        "             obj.print_data((list(lexnlp.extract.en.ratios.get_ratios(content))),\"ratios\")\n",
        "             obj.print_data((list(lexnlp.extract.en.regulations.get_regulations(content))),\"regulations\")\n",
        "             obj.print_data((list(lexnlp.extract.en.trademarks.get_trademarks(content))),\"trademarks\")\n",
        "             obj.print_data((list(lexnlp.extract.en.urls.get_urls(content))),\"urls\")\n",
        "\n",
        "      def print_data(self,list,list_name):\n",
        "        if list != []:\n",
        "          print(\"list of \"+list_name+\" is: \"+str(list))   \n",
        "        else:\n",
        "          print(\"list of \"+list_name+\" is: \"+ str(None)) \n",
        "\n",
        "obj=In_Class_5()\n",
        "obj.get_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [Connecting to security.u\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Connecting to security.ubu\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rHit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \r0% [Waiting for headers] [Waiting for headers] [Waiting for headers]\r0% [Release.gpg gpgv 564 B] [Waiting for headers] [Waiting for headers] [Waitin\r                                                                               \rHit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\r                                                                               \r0% [Release.gpg gpgv 564 B] [Waiting for headers] [Waiting for headers]\r                                                                       \rHit:7 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "\r0% [Release.gpg gpgv 564 B] [Waiting for headers] [Waiting for headers]\r                                                                       \rHit:8 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "\r0% [Release.gpg gpgv 564 B] [Waiting for headers] [Connecting to ppa.launchpad.\r                                                                               \r0% [Waiting for headers] [Connecting to ppa.launchpad.net (91.189.95.83)]\r0% [Release.gpg gpgv 564 B] [Waiting for headers] [Connecting to ppa.launchpad.\r                                                                               \rHit:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "\r0% [Release.gpg gpgv 564 B] [Waiting for headers] [Connecting to ppa.launchpad.\r                                                                               \r0% [Waiting for headers] [Waiting for headers]\r0% [6 InRelease gpgv 242 kB] [Waiting for headers] [Waiting for headers]\r                                                                        \rHit:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:13 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "chromium-chromedriver is already the newest version (80.0.3987.87-0ubuntu0.18.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 90 not upgraded.\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.6/dist-packages (3.141.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "list of amounts is: [5.0, 740.0, 1843.0, 2.0, 1.0, 4.0, 2.0, 1821.0, 5.0, 1.0, 1840.0, 3777, 80.0, 100.0, 30, 1839.0, 741.0, 22, 1840.0, 14000, 120, 1, 1840.0, 3, 4, 1, 1.0, 1840.0, 2.0, 1.0, 361.0, 1.0, 307.0, 6.0, 604.0, 1.0, 2.0, 418.0, 422.0, 7.0, 34.0, 41.0, 167.0, 742.0, 3.0, 112.0, 207.0, 3.0, 338.0, 424.0, 5.0, 26.0, 13.0, 235.0, 8.0, 693.0, 4.0, 1821.0, 167.0, 2.0, 2.0, 216.0, 3.0, 66.0, 4.0, 130.0, 29.0, 2.0, 241.0, 2.0, 332.0, 2.0, 422.0, 9.0, 112.0, 743.0, 9.0, 39.0, 14000, 1840.0, 744.0, 5.0, 182.0, 3.0, 368.0, 1.0, 397.0, 6.0, 604.0, 1, 1821.0, 167.0, 745.0, 4.0, 746.0, 4.0, 210.0, 46.0, 747.0, 5.0, 5.0, 740.0, 1843.0, 284.0, 2019.0, 9.0, 1.0, 55.0, 266.0, 271.0, 1876.0, 2.0, 47.0, 362.0, 376.0, 1872.0, 3.0, 45.0, 329.0, 334.0, 1871.0, 4.0, 31.0, 526.0, 527.0, 1858.0, 5.0, 21.0, 333.0, 335.0, 1852.0, 6.0, 8.0, 145.0, 147.0, 1857.0, 7.0, 65.0, 256.0, 258.0, 3, 1880.0, 8.0, 4.0, 913.0, 914.0, 1887.0, 9.0, 103.0, 464.0, 1936.0, 3.0, 1.0, 9.0, 39.0, 1828.0, 2.0, 2.0, 5.0, 182.0, 1837.0, 2.0, 3.0, 9.0, 108.0, 1812.0, 6, 1, 2.0]\n",
            "list of acts is: None\n",
            "list of citation is: [(5, 'Ala.', 'Alabama Reports', 740, None, None, None), (5, 'Ala.', 'Alabama Reports', 740, '1843', None, None), (55, 'Ala.', 'Alabama Reports', 266, '271', None, None), (47, 'Ala.', 'Alabama Reports', 362, '376', None, None), (45, 'Ala.', 'Alabama Reports', 329, '334', None, None), (31, 'Ala.', 'Alabama Reports', 526, '527', None, None), (21, 'Ala.', 'Alabama Reports', 333, '335', None, None), (8, 'Cal.', 'California Reports', 145, '147', None, None), (65, 'Ala.', 'Alabama Reports', 256, '258', None, None), (4, 'S.W.', 'South Western Reporter', 913, '914', None, None), (103, 'A.L.R.', 'American Law Reports', 464, None, None, None), (9, 'Cow.', \"Cowen's Reports\", 39, None, None, None), (5, 'Port.', 'Alabama Reports, Porter', 182, None, None, None), (9, 'Johns.', \"Johnson's Reports\", 108, None, None, None)]\n",
            "list of companies is: [Lehman, Durr Co, (17982, 18000)]\n",
            "list of conditions is: [('until', '[2]\\nCreditors’ Remedies\\nLien and Priority\\nUnder St.1821, prohibiting a levy on a crop', ''), ('until', 'on a growing crop, nor does such lien attach', ''), ('if', 'It was proved by the claimants, by the production of a written contract, that Harrison, on the twenty-second of May, 1840, in consideration that the claimants were involved, as indorsers for Burton & Harrison of Sumter county, and were then exposed to an execution, amounting to upwards of fourteen thousand dollars, bargained and sold to the claimants all his growing crop of cotton &c., consisting of one hundred and twenty acres, &c. Allen Harrison promised and obliged himself to give up his crop to the use of the claimants at any time to save them from suffering as his indorsers;', ''), ('when', 'The claimants came from Tennessee, (where they resided) about the first of September, 1840, bringing with them three or four white laborers, and took possession of the crop and slaves, and with the latter, and white laborers, gathered the cotton, prepared it for market, and', ''), ('if', 'The court charged the jury, that the plaintiff had no lien by virtue of his judgment, and execution on the growing crop; that Harrison had a right to convey it, without being in any manner restrained by them; that the writing adduced, was a sale of the crop, but', ''), ('when', 'it was not, and the lien of the fieri facias would have attached upon it,', ''), ('if', 'gathered, yet', ''), ('not subject to', 'the claimants obtained possession on the first of September, and controlled the gathering of the crop, then no lien attached, and it was', ''), ('until', 'Rep, 693;] and', ''), ('until', '167,] which declares it to be lawful to levy an execution on a planted crop,', ''), ('if', 'It is admitted that the contract between the defendant in execution, and the claimants, was in good faith,', ''), ('when', 'The defendant in execution might at any time have divested the interest which the contract vested in the claimants, by discharging their liability as his indorsers, or a judgment creditor might have satisfied the lien, and', ''), ('unless', 'We will then consider the writing under which the claimants assert a right, as a mortgage with a power to take possession any time during the year,', ''), ('if', 'Conceding the truth of the facts stated in the bill of exceptions, and we think it will not follow, that the possession of the claimants is a nullity, and that the case must be considered as', ''), ('if', 'The contract contains an express undertaking to give up the crop at any time the claimants might require it for their indemnity, and', ''), ('if', 'they took possession of it in the absence of the grantor, (though without his consent,)', ''), ('if', 'he subsequently acquiesced in it, the inference would be,', ''), ('subject to', 'Mr. Dane, in remarking upon this point, says, “The American editor of Bacon’s Abridgment, says, ‘Wheat growing in the ground is a chattel, and', ''), ('until', 'The first section of the act of 1821, “To prevent sheriffs and other officers from levying executions in certain cases, enacts, that “It shall not be lawful for any sheriff or other officer, to levy a writ of fieri facias or other execution on the planted crop of a debtor, or person against whom an execution may issue,', ''), ('until', 'Now here is an express inhibition to levy an execution on a crop while it remains on, or in the ground, and', ''), ('until', 'If so, the act cited, will only have the effect of keeping the right to levy it in abeyance', ''), ('if', 'The lien and the right to levy are intimately connected, and', ''), ('until', 'That it was competent for the legislature to have made it unlawful to levy an execution on particular property,', ''), ('until', 'If the object was merely to suspend the sale,', ''), ('as soon as', 'The idea that the lien attached upon the planted crop', ''), ('until', 'the execution was delivered to the sheriff, though the right to levy it was postponed', ''), ('if', 'They do not refer to the lien,', ''), ('until', 'they did they would postpone it', ''), ('until', 'the crop was gathered; but it is the levy they relate to and postpone', ''), ('until', '**4 The right to levy an execution on a planted crop, then, being expressly taken away by the statute, the lien which is connected with and consequent upon that right, never attaches', ''), ('if', 'The circuit judge may have mistaken the law in supposing that the contract was a sale, but', ''), ('when', 'There is no assumption of any material fact in the charge; but the possession of the claimant, the time', ''), ('if', 'acquired, the gathering of the crop, &c., are all referred to the determination of the jury; who are instructed,', ''), ('until', '**4 The statute which presents the question before the court is, that “it shall not be lawful for any sheriff or other officer to levy a writ of fieei facias or other execution, on the planted crop of a debtor, or person against whom an execution may issue,', ''), ('subject to', 'The policy of the State, as indicated by these statutes, is undeniably that all the property of a debtor, real and personal, to which he has a legal title, shall be', ''), ('until', 'The mischief which the statute designed to remedy was, the sacrifice which would be necessarily made by the sale of an immature crop: the statute enables the debtor to retain it', ''), ('if', '**5', ''), ('until', 'The sheriff is forbidden to levy on a “planted crop”', ''), ('if', 'Now,', ''), ('until', 'This, I feel a thorough conviction, was not the intention of the legislature; but that it was to secure him from loss, by prohibiting a levy and sale of the crop,', ''), ('when', 'it was gathered,', ''), ('subject to', 'Growing crops as', ''), ('subject to', '464\\nGenerally, at common law, growing crops raised by annual planting, while still attached to the soil, are regarded as personal chattels,', ''), ('where', 'And', '')]\n",
            "list of constraints is: [('after', 'on a growing crop, nor does such lien attach until', ''), ('after', '', ' and that alias and pluries fieri facias’, issued regularly up to the time levy was made; that the cotton levied on was growed on the plantation of harrison, and cultivated by the hands in his service.'), ('first of', 'the claimants came from tennessee, (where they resided) about the', ''), ('first of', 'the court charged the jury, that the plaintiff had no lien by virtue of his judgment, and execution on the growing crop; that harrison had a right to convey it, without being in any manner restrained by them; that the writing adduced, was a sale of the crop, but if it was not, and the lien of the fieri facias would have attached upon it, when gathered, yet if the claimants obtained possession on the', ''), ('after', 'it merely inhibits the levy, but the lien attaches, and a levy and sale may be made', ''), ('more than', 'taking this to be clear *744 law, and it will be seen, that the defendant in execution at the time of the levy had nothing', ''), ('before', 'it has been frequently mooted whether, at common law, corn, &c.,', ''), ('before', '**4 the statute which presents the question', ''), ('after', 'now, if the view taken by the majority of the court, is correct, the right secured to the plaintiff in execution, of levying on the crop', ''), ('before', 'tried', ''), ('before', 'tried', ''), ('before', 'tried', ''), ('before', 'tried', ''), ('before', 'tried', ''), ('before', 'tried', '')]\n",
            "list of copyright is: [('©', '2019', 'Thomson Reuters. No')]\n",
            "list of courts is:\n",
            "entity= (1, 'United States Supreme Court', 0, [('United States Supreme Court', None, False, None, ' united states supreme court '), ('Supreme Court', None, False, None, ' supreme court '), (' SCOTUS', None, False, None, ' scotus ')])\n",
            "list of geographical entities is:\n",
            "entity= (254, 'Alabama', 'US States', [('Alabama', None, False, None, ' alabama ')])\n",
            "entity= (296, 'Tennessee', 'US States', [('Tennessee', None, False, None, ' tennessee ')])\n",
            "entity= (240, 'United States', 'Countries', [('United States', None, False, None, ' united states ')])\n",
            "list of cusip is: None\n",
            "list of dates is: [datetime.date(2020, 6, 1), datetime.date(1840, 11, 1), datetime.date(1839, 10, 1), datetime.date(1840, 5, 1), datetime.date(1840, 9, 1), datetime.date(1840, 5, 1), datetime.date(1840, 5, 1), datetime.date(2020, 12, 1), datetime.date(2020, 12, 1), datetime.date(2020, 1, 1), datetime.date(2020, 1, 1), datetime.date(2020, 1, 1), datetime.date(2020, 3, 21), datetime.date(2020, 6, 1), datetime.date(2020, 7, 1), datetime.date(2020, 11, 1), datetime.date(1887, 5, 1)]\n",
            "list of definitions is: None\n",
            "list of distances is: None\n",
            "list of durations is: [('second', 20.0, 0.00023148148148148146), ('year', 6.0, 2190.0)]\n",
            "list of money is: [(100.0, 'USD'), (14000, 'USD'), (14000, 'USD')]\n",
            "list of percents is: None\n",
            "list of PII is: None\n",
            "list of ratios is: None\n",
            "list of regulations is: None\n",
            "list of trademarks is: None\n",
            "list of urls is: None\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}